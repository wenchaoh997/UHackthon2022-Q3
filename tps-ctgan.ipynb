{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b38a82f9-e25f-4c0d-a8ae-cc0e882431e8","_uuid":"4905db03-f142-438b-84d2-e5485af03531","execution":{"iopub.execute_input":"2022-04-09T16:38:16.681004Z","iopub.status.busy":"2022-04-09T16:38:16.680243Z","iopub.status.idle":"2022-04-09T16:38:16.686162Z","shell.execute_reply":"2022-04-09T16:38:16.685414Z","shell.execute_reply.started":"2022-04-09T16:38:16.680892Z"},"trusted":true},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import numpy as np\n","import pandas as pd "]},{"cell_type":"markdown","metadata":{},"source":["# CTGAN"]},{"cell_type":"code","execution_count":8,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-04-09T16:39:08.777972Z","iopub.status.busy":"2022-04-09T16:39:08.777535Z","iopub.status.idle":"2022-04-09T16:39:11.323655Z","shell.execute_reply":"2022-04-09T16:39:11.322660Z","shell.execute_reply.started":"2022-04-09T16:39:08.777939Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["import torch\n","from torch import optim\n","from torch.nn import functional\n","from torch.nn import BatchNorm1d, Dropout, LeakyReLU, Linear, Module, ReLU, Sequential\n","from sklearn.exceptions import ConvergenceWarning\n","from sklearn.mixture import BayesianGaussianMixture\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.utils._testing import ignore_warnings\n","\n","class ConditionalGenerator(object):\n","    def __init__(self, data, output_info, log_frequency):\n","        self.model = []\n","\n","        start = 0\n","        skip = False\n","        max_interval = 0\n","        counter = 0\n","        for item in output_info:\n","            if item[1] == 'tanh':\n","                start += item[0]\n","                skip = True\n","                continue\n","\n","            elif item[1] == 'softmax':\n","                if skip:\n","                    skip = False\n","                    start += item[0]\n","                    continue\n","\n","                end = start + item[0]\n","                max_interval = max(max_interval, end - start)\n","                counter += 1\n","                self.model.append(np.argmax(data[:, start:end], axis=-1))\n","                start = end\n","\n","            else:\n","                assert 0\n","\n","        assert start == data.shape[1]\n","\n","        self.interval = []\n","        self.n_col = 0\n","        self.n_opt = 0\n","        skip = False\n","        start = 0\n","        self.p = np.zeros((counter, max_interval))\n","        for item in output_info:\n","            if item[1] == 'tanh':\n","                skip = True\n","                start += item[0]\n","                continue\n","            elif item[1] == 'softmax':\n","                if skip:\n","                    start += item[0]\n","                    skip = False\n","                    continue\n","                end = start + item[0]\n","                tmp = np.sum(data[:, start:end], axis=0)\n","                if log_frequency:\n","                    tmp = np.log(tmp + 1)\n","                tmp = tmp / np.sum(tmp)\n","                self.p[self.n_col, :item[0]] = tmp\n","                self.interval.append((self.n_opt, item[0]))\n","                self.n_opt += item[0]\n","                self.n_col += 1\n","                start = end\n","            else:\n","                assert 0\n","\n","        self.interval = np.asarray(self.interval)\n","\n","    def random_choice_prob_index(self, idx):\n","        a = self.p[idx]\n","        r = np.expand_dims(np.random.rand(a.shape[0]), axis=1)\n","        return (a.cumsum(axis=1) > r).argmax(axis=1)\n","\n","    def sample(self, batch):\n","        if self.n_col == 0:\n","            return None\n","\n","        batch = batch\n","        idx = np.random.choice(np.arange(self.n_col), batch)\n","\n","        vec1 = np.zeros((batch, self.n_opt), dtype='float32')\n","        mask1 = np.zeros((batch, self.n_col), dtype='float32')\n","        mask1[np.arange(batch), idx] = 1\n","        opt1prime = self.random_choice_prob_index(idx)\n","        opt1 = self.interval[idx, 0] + opt1prime\n","        vec1[np.arange(batch), opt1] = 1\n","\n","        return vec1, mask1, idx, opt1prime\n","\n","    def sample_zero(self, batch):\n","        if self.n_col == 0:\n","            return None\n","\n","        vec = np.zeros((batch, self.n_opt), dtype='float32')\n","        idx = np.random.choice(np.arange(self.n_col), batch)\n","        for i in range(batch):\n","            col = idx[i]\n","            pick = int(np.random.choice(self.model[col]))\n","            vec[i, pick + self.interval[col, 0]] = 1\n","\n","        return vec\n","\n","class Discriminator(Module):\n","\n","    def calc_gradient_penalty(self, real_data, fake_data, device='cpu', pac=10, lambda_=10):\n","\n","        alpha = torch.rand(real_data.size(0) // pac, 1, 1, device=device)\n","        alpha = alpha.repeat(1, pac, real_data.size(1))\n","        alpha = alpha.view(-1, real_data.size(1))\n","\n","        interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n","\n","        disc_interpolates = self(interpolates)\n","\n","        gradients = torch.autograd.grad(\n","            outputs=disc_interpolates, inputs=interpolates,\n","            grad_outputs=torch.ones(disc_interpolates.size(), device=device),\n","            create_graph=True, retain_graph=True, only_inputs=True\n","        )[0]\n","\n","        gradient_penalty = ((\n","            gradients.view(-1, pac * real_data.size(1)).norm(2, dim=1) - 1\n","        ) ** 2).mean() * lambda_\n","\n","        return gradient_penalty\n","\n","    def __init__(self, input_dim, dis_dims, pack=10):\n","        super(Discriminator, self).__init__()\n","        dim = input_dim * pack\n","        self.pack = pack\n","        self.packdim = dim\n","        seq = []\n","        for item in list(dis_dims):\n","            seq += [Linear(dim, item), LeakyReLU(0.2), Dropout(0.5)]\n","            dim = item\n","\n","        seq += [Linear(dim, 1)]\n","        self.seq = Sequential(*seq)\n","\n","    def forward(self, input):\n","        assert input.size()[0] % self.pack == 0\n","        return self.seq(input.view(-1, self.packdim))\n","\n","\n","class Residual(Module):\n","    def __init__(self, i, o):\n","        super(Residual, self).__init__()\n","        self.fc = Linear(i, o)\n","        self.bn = BatchNorm1d(o)\n","        self.relu = ReLU()\n","\n","    def forward(self, input):\n","        out = self.fc(input)\n","        out = self.bn(out)\n","        out = self.relu(out)\n","        return torch.cat([out, input], dim=1)\n","\n","\n","class Generator(Module):\n","    def __init__(self, embedding_dim, gen_dims, data_dim):\n","        super(Generator, self).__init__()\n","        dim = embedding_dim\n","        seq = []\n","        for item in list(gen_dims):\n","            seq += [Residual(dim, item)]\n","            dim += item\n","        seq.append(Linear(dim, data_dim))\n","        self.seq = Sequential(*seq)\n","\n","    def forward(self, input):\n","        data = self.seq(input)\n","        return data\n","\n","class Sampler(object):\n","    \"\"\"docstring for Sampler.\"\"\"\n","\n","    def __init__(self, data, output_info):\n","        super(Sampler, self).__init__()\n","        self.data = data\n","        self.model = []\n","        self.n = len(data)\n","\n","        st = 0\n","        skip = False\n","        for item in output_info:\n","            if item[1] == 'tanh':\n","                st += item[0]\n","                skip = True\n","            elif item[1] == 'softmax':\n","                if skip:\n","                    skip = False\n","                    st += item[0]\n","                    continue\n","\n","                ed = st + item[0]\n","                tmp = []\n","                for j in range(item[0]):\n","                    tmp.append(np.nonzero(data[:, st + j])[0])\n","\n","                self.model.append(tmp)\n","                st = ed\n","            else:\n","                assert 0\n","\n","        assert st == data.shape[1]\n","\n","    def sample(self, n, col, opt):\n","        if col is None:\n","            idx = np.random.choice(np.arange(self.n), n)\n","            return self.data[idx]\n","\n","        idx = []\n","        for c, o in zip(col, opt):\n","            idx.append(np.random.choice(self.model[c][o]))\n","\n","        return self.data[idx]\n","\n","class DataTransformer(object):\n","    \"\"\"Data Transformer.\n","    Model continuous columns with a BayesianGMM and normalized to a scalar\n","    [0, 1] and a vector.\n","    Discrete columns are encoded using a scikit-learn OneHotEncoder.\n","    Args:\n","        n_cluster (int):\n","            Number of modes.\n","        epsilon (float):\n","            Epsilon value.\n","    \"\"\"\n","\n","    def __init__(self, n_clusters=10, epsilon=0.005):\n","        self.n_clusters = n_clusters\n","        self.epsilon = epsilon\n","\n","    @ignore_warnings(category=ConvergenceWarning)\n","    def _fit_continuous(self, column, data):\n","        gm = BayesianGaussianMixture(\n","            self.n_clusters,\n","            weight_concentration_prior_type='dirichlet_process',\n","            weight_concentration_prior=0.001,\n","            n_init=1\n","        )\n","        gm.fit(data)\n","        components = gm.weights_ > self.epsilon\n","        num_components = components.sum()\n","\n","        return {\n","            'name': column,\n","            'model': gm,\n","            'components': components,\n","            'output_info': [(1, 'tanh'), (num_components, 'softmax')],\n","            'output_dimensions': 1 + num_components,\n","        }\n","\n","    def _fit_discrete(self, column, data):\n","        ohe = OneHotEncoder(sparse=False)\n","        ohe.fit(data)\n","        categories = len(ohe.categories_[0])\n","\n","        return {\n","            'name': column,\n","            'encoder': ohe,\n","            'output_info': [(categories, 'softmax')],\n","            'output_dimensions': categories\n","        }\n","\n","    def fit(self, data, discrete_columns=tuple()):\n","        self.output_info = []\n","        self.output_dimensions = 0\n","\n","        if not isinstance(data, pd.DataFrame):\n","            self.dataframe = False\n","            data = pd.DataFrame(data)\n","        else:\n","            self.dataframe = True\n","\n","        self.meta = []\n","        for column in data.columns:\n","            column_data = data[[column]].values\n","            if column in discrete_columns:\n","                meta = self._fit_discrete(column, column_data)\n","            else:\n","                meta = self._fit_continuous(column, column_data)\n","\n","            self.output_info += meta['output_info']\n","            self.output_dimensions += meta['output_dimensions']\n","            self.meta.append(meta)\n","\n","    def _transform_continuous(self, column_meta, data):\n","        components = column_meta['components']\n","        model = column_meta['model']\n","\n","        means = model.means_.reshape((1, self.n_clusters))\n","        stds = np.sqrt(model.covariances_).reshape((1, self.n_clusters))\n","        features = (data - means) / (4 * stds)\n","\n","        probs = model.predict_proba(data)\n","\n","        n_opts = components.sum()\n","        features = features[:, components]\n","        probs = probs[:, components]\n","\n","        opt_sel = np.zeros(len(data), dtype='int')\n","        for i in range(len(data)):\n","            pp = probs[i] + 1e-6\n","            pp = pp / pp.sum()\n","            opt_sel[i] = np.random.choice(np.arange(n_opts), p=pp)\n","\n","        idx = np.arange((len(features)))\n","        features = features[idx, opt_sel].reshape([-1, 1])\n","        features = np.clip(features, -.99, .99)\n","\n","        probs_onehot = np.zeros_like(probs)\n","        probs_onehot[np.arange(len(probs)), opt_sel] = 1\n","        return [features, probs_onehot]\n","\n","    def _transform_discrete(self, column_meta, data):\n","        encoder = column_meta['encoder']\n","        return encoder.transform(data)\n","\n","    def transform(self, data):\n","        if not isinstance(data, pd.DataFrame):\n","            data = pd.DataFrame(data)\n","\n","        values = []\n","        for meta in self.meta:\n","            column_data = data[[meta['name']]].values\n","            if 'model' in meta:\n","                values += self._transform_continuous(meta, column_data)\n","            else:\n","                values.append(self._transform_discrete(meta, column_data))\n","\n","        return np.concatenate(values, axis=1).astype(float)\n","\n","    def _inverse_transform_continuous(self, meta, data, sigma):\n","        model = meta['model']\n","        components = meta['components']\n","\n","        u = data[:, 0]\n","        v = data[:, 1:]\n","\n","        if sigma is not None:\n","            u = np.random.normal(u, sigma)\n","\n","        u = np.clip(u, -1, 1)\n","        v_t = np.ones((len(data), self.n_clusters)) * -100\n","        v_t[:, components] = v\n","        v = v_t\n","        means = model.means_.reshape([-1])\n","        stds = np.sqrt(model.covariances_).reshape([-1])\n","        p_argmax = np.argmax(v, axis=1)\n","        std_t = stds[p_argmax]\n","        mean_t = means[p_argmax]\n","        column = u * 4 * std_t + mean_t\n","\n","        return column\n","\n","    def _inverse_transform_discrete(self, meta, data):\n","        encoder = meta['encoder']\n","        return encoder.inverse_transform(data)\n","\n","    def inverse_transform(self, data, sigmas):\n","        start = 0\n","        output = []\n","        column_names = []\n","        for meta in self.meta:\n","            dimensions = meta['output_dimensions']\n","            columns_data = data[:, start:start + dimensions]\n","\n","            if 'model' in meta:\n","                sigma = sigmas[start] if sigmas else None\n","                inverted = self._inverse_transform_continuous(meta, columns_data, sigma)\n","            else:\n","                inverted = self._inverse_transform_discrete(meta, columns_data)\n","\n","            output.append(inverted)\n","            column_names.append(meta['name'])\n","            start += dimensions\n","\n","        output = np.column_stack(output)\n","        if self.dataframe:\n","            output = pd.DataFrame(output, columns=column_names)\n","\n","        return output\n","\n","class CTGANSynthesizer(object):\n","    \"\"\"Conditional Table GAN Synthesizer.\n","    This is the core class of the CTGAN project, where the different components\n","    are orchestrated together.\n","    For more details about the process, please check the [Modeling Tabular data using\n","    Conditional GAN](https://arxiv.org/abs/1907.00503) paper.\n","    Args:\n","        embedding_dim (int):\n","            Size of the random sample passed to the Generator. Defaults to 128.\n","        gen_dim (tuple or list of ints):\n","            Size of the output samples for each one of the Residuals. A Residual Layer\n","            will be created for each one of the values provided. Defaults to (256, 256).\n","        dis_dim (tuple or list of ints):\n","            Size of the output samples for each one of the Discriminator Layers. A Linear Layer\n","            will be created for each one of the values provided. Defaults to (256, 256).\n","        l2scale (float):\n","            Wheight Decay for the Adam Optimizer. Defaults to 1e-6.\n","        batch_size (int):\n","            Number of data samples to process in each step.\n","    \"\"\"\n","\n","    def __init__(self, embedding_dim=128, gen_dim=(256, 256), dis_dim=(256, 256),\n","                 l2scale=1e-6, batch_size=500):\n","\n","        self.embedding_dim = embedding_dim\n","        self.gen_dim = gen_dim\n","        self.dis_dim = dis_dim\n","\n","        self.l2scale = l2scale\n","        self.batch_size = batch_size\n","        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","        self.trained_epoches = 0\n","\n","    def _apply_activate(self, data):\n","        data_t = []\n","        st = 0\n","        for item in self.transformer.output_info:\n","            if item[1] == 'tanh':\n","                ed = st + item[0]\n","                data_t.append(torch.tanh(data[:, st:ed]))\n","                st = ed\n","            elif item[1] == 'softmax':\n","                ed = st + item[0]\n","                data_t.append(functional.gumbel_softmax(data[:, st:ed], tau=0.2))\n","                st = ed\n","            else:\n","                assert 0\n","\n","        return torch.cat(data_t, dim=1)\n","\n","    def _cond_loss(self, data, c, m):\n","        loss = []\n","        st = 0\n","        st_c = 0\n","        skip = False\n","        for item in self.transformer.output_info:\n","            if item[1] == 'tanh':\n","                st += item[0]\n","                skip = True\n","\n","            elif item[1] == 'softmax':\n","                if skip:\n","                    skip = False\n","                    st += item[0]\n","                    continue\n","\n","                ed = st + item[0]\n","                ed_c = st_c + item[0]\n","                tmp = functional.cross_entropy(\n","                    data[:, st:ed],\n","                    torch.argmax(c[:, st_c:ed_c], dim=1),\n","                    reduction='none'\n","                )\n","                loss.append(tmp)\n","                st = ed\n","                st_c = ed_c\n","\n","            else:\n","                assert 0\n","\n","        loss = torch.stack(loss, dim=1)\n","\n","        return (loss * m).sum() / data.size()[0]\n","\n","    def fit(self, train_data, discrete_columns=tuple(), epochs=300, log_frequency=True):\n","        \"\"\"Fit the CTGAN Synthesizer models to the training data.\n","        Args:\n","            train_data (numpy.ndarray or pandas.DataFrame):\n","                Training Data. It must be a 2-dimensional numpy array or a\n","                pandas.DataFrame.\n","            discrete_columns (list-like):\n","                List of discrete columns to be used to generate the Conditional\n","                Vector. If ``train_data`` is a Numpy array, this list should\n","                contain the integer indices of the columns. Otherwise, if it is\n","                a ``pandas.DataFrame``, this list should contain the column names.\n","            epochs (int):\n","                Number of training epochs. Defaults to 300.\n","            log_frequency (boolean):\n","                Whether to use log frequency of categorical levels in conditional\n","                sampling. Defaults to ``True``.\n","        \"\"\"\n","\n","        if not hasattr(self, \"transformer\"):\n","            self.transformer = DataTransformer()\n","            self.transformer.fit(train_data, discrete_columns)\n","        train_data = self.transformer.transform(train_data)\n","\n","        data_sampler = Sampler(train_data, self.transformer.output_info)\n","\n","        data_dim = self.transformer.output_dimensions\n","\n","        if not hasattr(self, \"cond_generator\"):\n","            self.cond_generator = ConditionalGenerator(\n","                train_data,\n","                self.transformer.output_info,\n","                log_frequency\n","            )\n","\n","        if not hasattr(self, \"generator\"):\n","            self.generator = Generator(\n","                self.embedding_dim + self.cond_generator.n_opt,\n","                self.gen_dim,\n","                data_dim\n","            ).to(self.device)\n","\n","        if not hasattr(self, \"discriminator\"):\n","            self.discriminator = Discriminator(\n","                data_dim + self.cond_generator.n_opt,\n","                self.dis_dim\n","            ).to(self.device)\n","\n","        if not hasattr(self, \"optimizerG\"):\n","            self.optimizerG = optim.Adam(\n","                self.generator.parameters(), lr=2e-4, betas=(0.5, 0.9),\n","                weight_decay=self.l2scale\n","            )\n","\n","        if not hasattr(self, \"optimizerD\"):\n","            self.optimizerD = optim.Adam(\n","                self.discriminator.parameters(), lr=2e-4, betas=(0.5, 0.9))\n","\n","        assert self.batch_size % 2 == 0\n","        mean = torch.zeros(self.batch_size, self.embedding_dim, device=self.device)\n","        std = mean + 1\n","\n","        steps_per_epoch = max(len(train_data) // self.batch_size, 1)\n","        for i in range(epochs):\n","            self.trained_epoches += 1\n","            for id_ in range(steps_per_epoch):\n","                fakez = torch.normal(mean=mean, std=std)\n","\n","                condvec = self.cond_generator.sample(self.batch_size)\n","                if condvec is None:\n","                    c1, m1, col, opt = None, None, None, None\n","                    real = data_sampler.sample(self.batch_size, col, opt)\n","                else:\n","                    c1, m1, col, opt = condvec\n","                    c1 = torch.from_numpy(c1).to(self.device)\n","                    m1 = torch.from_numpy(m1).to(self.device)\n","                    fakez = torch.cat([fakez, c1], dim=1)\n","\n","                    perm = np.arange(self.batch_size)\n","                    np.random.shuffle(perm)\n","                    real = data_sampler.sample(self.batch_size, col[perm], opt[perm])\n","                    c2 = c1[perm]\n","\n","                fake = self.generator(fakez)\n","                fakeact = self._apply_activate(fake)\n","\n","                real = torch.from_numpy(real.astype('float32')).to(self.device)\n","\n","                if c1 is not None:\n","                    fake_cat = torch.cat([fakeact, c1], dim=1)\n","                    real_cat = torch.cat([real, c2], dim=1)\n","                else:\n","                    real_cat = real\n","                    fake_cat = fake\n","\n","                y_fake = self.discriminator(fake_cat)\n","                y_real = self.discriminator(real_cat)\n","\n","                pen = self.discriminator.calc_gradient_penalty(\n","                    real_cat, fake_cat, self.device)\n","                loss_d = -(torch.mean(y_real) - torch.mean(y_fake))\n","\n","                self.optimizerD.zero_grad()\n","                pen.backward(retain_graph=True)\n","                loss_d.backward()\n","                self.optimizerD.step()\n","\n","                fakez = torch.normal(mean=mean, std=std)\n","                condvec = self.cond_generator.sample(self.batch_size)\n","\n","                if condvec is None:\n","                    c1, m1, col, opt = None, None, None, None\n","                else:\n","                    c1, m1, col, opt = condvec\n","                    c1 = torch.from_numpy(c1).to(self.device)\n","                    m1 = torch.from_numpy(m1).to(self.device)\n","                    fakez = torch.cat([fakez, c1], dim=1)\n","\n","                fake = self.generator(fakez)\n","                fakeact = self._apply_activate(fake)\n","\n","                if c1 is not None:\n","                    y_fake = self.discriminator(torch.cat([fakeact, c1], dim=1))\n","                else:\n","                    y_fake = self.discriminator(fakeact)\n","\n","                if condvec is None:\n","                    cross_entropy = 0\n","                else:\n","                    cross_entropy = self._cond_loss(fake, c1, m1)\n","\n","                loss_g = -torch.mean(y_fake) + cross_entropy\n","\n","                self.optimizerG.zero_grad()\n","                loss_g.backward()\n","                self.optimizerG.step()\n","\n","            l_average = (loss_g.item() + loss_d.item()) / 2\n","            print(\"Epoch %d, Loss G: %.4f, Loss D: %.4f, Loss A: %.4f\" %\n","                  (self.trained_epoches, loss_g.detach().cpu(), loss_d.detach().cpu(), l_average),\n","                  flush=True)\n","\n","    def sample(self, n, condition_column=None, condition_value=None):\n","        \"\"\"Sample data similar to the training data.\n","        Args:\n","            n (int):\n","                Number of rows to sample.\n","        Returns:\n","            numpy.ndarray or pandas.DataFrame\n","        \"\"\"\n","\n","        if condition_column is not None and condition_value is not None:\n","            condition_info = self.transformer.covert_column_name_value_to_id(\n","                condition_column, condition_value)\n","            global_condition_vec = self.cond_generator.generate_cond_from_condition_column_info(\n","                condition_info, self.batch_size)\n","        else:\n","            global_condition_vec = None\n","\n","        steps = n // self.batch_size + 1\n","        data = []\n","        for i in range(steps):\n","            mean = torch.zeros(self.batch_size, self.embedding_dim)\n","            std = mean + 1\n","            fakez = torch.normal(mean=mean, std=std).to(self.device)\n","\n","            if global_condition_vec is not None:\n","                condvec = global_condition_vec.copy()\n","            else:\n","                condvec = self.cond_generator.sample_zero(self.batch_size)\n","\n","            if condvec is None:\n","                pass\n","            else:\n","                c1 = condvec\n","                c1 = torch.from_numpy(c1).to(self.device)\n","                fakez = torch.cat([fakez, c1], dim=1)\n","\n","            fake = self.generator(fakez)\n","            fakeact = self._apply_activate(fake)\n","            data.append(fakeact.detach().cpu().numpy())\n","\n","        data = np.concatenate(data, axis=0)\n","        data = data[:n]\n","\n","        return self.transformer.inverse_transform(data, None)\n","\n","    def save(self, path):\n","        assert hasattr(self, \"generator\")\n","        assert hasattr(self, \"discriminator\")\n","        assert hasattr(self, \"transformer\")\n","\n","        # always save a cpu model.\n","        device_bak = self.device\n","        self.device = torch.device(\"cpu\")\n","        self.generator.to(self.device)\n","        self.discriminator.to(self.device)\n","\n","        torch.save(self, path)\n","\n","        self.device = device_bak\n","        self.generator.to(self.device)\n","        self.discriminator.to(self.device)\n","\n","    @classmethod\n","    def load(cls, path):\n","        model = torch.load(path)\n","        model.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","        model.generator.to(model.device)\n","        model.discriminator.to(model.device)\n","        return model"]},{"cell_type":"markdown","metadata":{},"source":["# UHackthon"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2022-04-09T17:05:17.674182Z","iopub.status.busy":"2022-04-09T17:05:17.673467Z","iopub.status.idle":"2022-04-09T17:05:17.678499Z","shell.execute_reply":"2022-04-09T17:05:17.677489Z","shell.execute_reply.started":"2022-04-09T17:05:17.674141Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","import gc"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2022-04-09T17:05:19.558252Z","iopub.status.busy":"2022-04-09T17:05:19.557897Z","iopub.status.idle":"2022-04-09T17:05:19.570403Z","shell.execute_reply":"2022-04-09T17:05:19.569312Z","shell.execute_reply.started":"2022-04-09T17:05:19.558218Z"},"trusted":true},"outputs":[],"source":["train_info = \"../input/uhackthon/train_info.csv\"\n","train_sales = \"../input/uhackthon/train_sales.csv\"\n","info = pd.read_csv(train_info)\n","sales = pd.read_csv(train_sales)"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2022-04-09T17:05:24.494509Z","iopub.status.busy":"2022-04-09T17:05:24.493916Z","iopub.status.idle":"2022-04-09T17:05:24.514918Z","shell.execute_reply":"2022-04-09T17:05:24.513878Z","shell.execute_reply.started":"2022-04-09T17:05:24.494472Z"},"trusted":true},"outputs":[],"source":["def bar_code_helper(x: int) -> int:\n","    ##All values are 690\n","    x = x // 1e10\n","    return x\n","\n","# for duplicated features\n","def duplicated_helper(df: pd.DataFrame) -> pd.DataFrame:\n","    idRecords = {}\n","    count = 0\n","    n = len(df)\n","    new_df = pd.DataFrame(columns=df.columns)\n","    for i in range(n):\n","        if df.loc[i, \"uuid\"] in idRecords.keys():\n","            assert new_df.loc[new_df[\"uuid\"]==df.loc[i, \"uuid\"], \"launch_date\"].values == df.loc[i, \"launch_date\"]\n","            idRecords[df.loc[i, \"uuid\"]] += 1\n","            # ingredient\n","            new_df.loc[new_df[\"uuid\"]==df.loc[i, \"uuid\"], \"ingredient\"] = list(\n","                set(df.loc[i, \"ingredient\"] + new_df.loc[new_df[\"uuid\"]==df.loc[i, \"uuid\"], \"ingredient\"])\n","            )\n","            # material_name\n","            new_df.loc[new_df[\"uuid\"]==df.loc[i, \"uuid\"], \"material_name\"] = list(\n","                set(new_df.loc[new_df[\"uuid\"]==df.loc[i, \"uuid\"], \"material_name\"] + \" \" + df.loc[i, \"material_name\"])\n","            )\n","            # material_name_zh\n","            new_df.loc[new_df[\"uuid\"]==df.loc[i, \"uuid\"], \"material_name_zh\"] = list(\n","                set(new_df.loc[new_df[\"uuid\"]==df.loc[i, \"uuid\"], \"material_name_zh\"] + \" \" + df.loc[i, \"material_name_zh\"])\n","            )\n","        else:\n","            new_df.loc[count,] = df.loc[i,].tolist()\n","            idRecords[df.loc[i, \"uuid\"]] = 1\n","            count += 1\n","    new_df[\"mentioned\"] = 1\n","    n = len(new_df)\n","    for i in range(n):\n","        new_df.loc[i, \"mentioned\"] = idRecords[new_df.loc[i, \"uuid\"]]\n","    return new_df\n","\n","def ingredient_helper(x: str) -> str:\n","    x = x[1:-1].replace(\"'\", \"\")\n","    x = x.replace(\"[\", \"\")\n","    return x.replace(\"]\", \"\")\n","\n","def launch_date_helper(x: str) -> int:\n","    yy, mm, dd = x.split(\"-\")\n","    return [int(yy), int(mm), int(dd)]\n","def mm_distance_helper1(source: float, target: int=1) -> float:\n","    return min(abs(source - target), abs(source - 12 - target))\n","def mm_distance_helper2(source: float, target: int=2) -> float:\n","    return min(abs(source - target), abs(source - 12 - target))\n","def mm_distance_helper10(source: float, target: int=10) -> float:\n","    return min(abs(source - target), abs(source - 12 - target))\n","def mm_distance_helper12(source: float, target: int=12) -> float:\n","    return min(abs(source - target), abs(source - 12 - target))\n","\n","def feature_engineering(info_df, sales_df, save=False) -> pd.DataFrame:\n","    # channel\n","    sales_df.loc[sales_df[\"channel\"]==\"EC\", \"channel\"] = 0\n","    sales_df.loc[sales_df[\"channel\"]==\"DT\", \"channel\"] = 1\n","#     sales_df.channel = sales_df.channel.astype(int)\n","    # category\n","    info_df.drop([\"category\"], axis=1, inplace=True)\n","    # brand\n","    info_df[[\"DOVE\", \"LUX\", \"VASELINE\"]] = 0\n","    info_df.DOVE = info_df.DOVE.astype(int)\n","    info_df.LUX = info_df.LUX.astype(int)\n","    info_df.VASELINE = info_df.VASELINE.astype(int)\n","    info_df.loc[info_df[\"brand\"]==\"DOVE\", \"DOVE\"] = 1\n","    info_df.loc[info_df[\"brand\"]==\"LUX\", \"LUX\"] = 1\n","    info_df.loc[info_df[\"brand\"]==\"VASELINE\", \"VASELINE\"] = 1\n","    # bar_code\n","    info_df[\"bar_code\"] = info_df[\"bar_code\"].apply(bar_code_helper)\n","    info_df.drop([\"bar_code\"], axis=1, inplace=True)\n","    # duplicated features\n","    info_df = duplicated_helper(info_df)\n","    # launch_date\n","    info_df[[\"yy\", \"mm\", \"dd\"]] = info_df[\"launch_date\"].apply(launch_date_helper).tolist()\n","    # sales_period_\n","    sales_df.loc[sales_df[\"sales_period_\"]==6, \"channel\"] = 0\n","    sales_df.loc[sales_df[\"sales_period_\"]==12, \"channel\"] = 1\n","    # distance to important month\n","    # im = [1, 2, 10, 12]\n","    info_df[\"dToM1\"] = info_df[\"mm\"].apply(mm_distance_helper1)\n","    info_df[\"dToM2\"] = info_df[\"mm\"].apply(mm_distance_helper2)\n","    info_df[\"dToM10\"] = info_df[\"mm\"].apply(mm_distance_helper10)\n","    info_df[\"dToM12\"] = info_df[\"mm\"].apply(mm_distance_helper12)\n","    # ingredient\n","    info_df[\"ingredient\"] = info_df[\"ingredient\"].apply(ingredient_helper)\n","    \n","    if save:\n","        info_df.to_csv(\"info_clean.csv\", index=False)\n","        sales_df.to_csv(\"sales_clean.csv\", index=False)\n","    return info_df, sales_df\n"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2022-04-09T17:05:24.517098Z","iopub.status.busy":"2022-04-09T17:05:24.516800Z","iopub.status.idle":"2022-04-09T17:05:24.698385Z","shell.execute_reply":"2022-04-09T17:05:24.697475Z","shell.execute_reply.started":"2022-04-09T17:05:24.517069Z"},"trusted":true},"outputs":[],"source":["new_info, new_sales = feature_engineering(info, sales, save=True)\n","data = pd.merge(new_sales, new_info, on=\"uuid\", how=\"left\")\n","noninfo = data[data[\"mentioned\"].isnull()].reset_index(drop=True)\n","data = data[data[\"mentioned\"]>0].reset_index(drop=True)"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2022-04-09T17:05:25.028594Z","iopub.status.busy":"2022-04-09T17:05:25.028235Z","iopub.status.idle":"2022-04-09T17:05:25.037228Z","shell.execute_reply":"2022-04-09T17:05:25.036183Z","shell.execute_reply.started":"2022-04-09T17:05:25.028538Z"},"trusted":true},"outputs":[],"source":["print(data.columns)\n","train_cols = ['channel', 'sales_period_', 'sales_value', \n","              'DOVE', 'LUX', 'VASELINE', 'mentioned', 'yy', 'mm', 'dd', \n","              'dToM1', 'dToM2', 'dToM10', 'dToM12']\n","train = data[train_cols]"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2022-04-09T17:15:28.674006Z","iopub.status.busy":"2022-04-09T17:15:28.673649Z","iopub.status.idle":"2022-04-09T17:15:28.680343Z","shell.execute_reply":"2022-04-09T17:15:28.679634Z","shell.execute_reply.started":"2022-04-09T17:15:28.673972Z"},"trusted":true},"outputs":[],"source":["discrete_cols = ['channel', 'DOVE', 'LUX', 'VASELINE']\n","# discrete_cols = ['channel', \"sales_period_\", 'DOVE', 'LUX', 'VASELINE']\n","discrete_cols"]},{"cell_type":"code","execution_count":33,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2022-04-09T17:15:32.288153Z","iopub.status.busy":"2022-04-09T17:15:32.287605Z","iopub.status.idle":"2022-04-09T17:15:53.645285Z","shell.execute_reply":"2022-04-09T17:15:53.644291Z","shell.execute_reply.started":"2022-04-09T17:15:32.288118Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["model = CTGANSynthesizer()\n","model.fit(train, discrete_cols, epochs=300)"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2022-04-09T17:15:53.648059Z","iopub.status.busy":"2022-04-09T17:15:53.647627Z","iopub.status.idle":"2022-04-09T17:15:53.706591Z","shell.execute_reply":"2022-04-09T17:15:53.705678Z","shell.execute_reply.started":"2022-04-09T17:15:53.648014Z"},"trusted":true},"outputs":[],"source":["from joblib import dump, load\n","\n","dump(model, 'ctgan_dump')\n","model = load('ctgan_dump')"]},{"cell_type":"code","execution_count":79,"metadata":{"execution":{"iopub.execute_input":"2022-04-09T17:54:27.505341Z","iopub.status.busy":"2022-04-09T17:54:27.505003Z","iopub.status.idle":"2022-04-09T17:54:27.555072Z","shell.execute_reply":"2022-04-09T17:54:27.554250Z","shell.execute_reply.started":"2022-04-09T17:54:27.505311Z"},"trusted":true},"outputs":[],"source":["samples = model.sample(500)\n","samples.to_csv('ctgan_aug.csv', index = False)"]},{"cell_type":"markdown","metadata":{},"source":["# training and testing sets"]},{"cell_type":"code","execution_count":80,"metadata":{"execution":{"iopub.execute_input":"2022-04-09T17:54:33.087007Z","iopub.status.busy":"2022-04-09T17:54:33.086455Z","iopub.status.idle":"2022-04-09T17:54:33.095650Z","shell.execute_reply":"2022-04-09T17:54:33.094754Z","shell.execute_reply.started":"2022-04-09T17:54:33.086967Z"},"trusted":true},"outputs":[],"source":["print(samples.columns)\n","samples.dtypes"]},{"cell_type":"code","execution_count":81,"metadata":{"execution":{"iopub.execute_input":"2022-04-09T17:54:33.771139Z","iopub.status.busy":"2022-04-09T17:54:33.770596Z","iopub.status.idle":"2022-04-09T17:54:33.776855Z","shell.execute_reply":"2022-04-09T17:54:33.775696Z","shell.execute_reply.started":"2022-04-09T17:54:33.771103Z"},"trusted":true},"outputs":[],"source":["train = data[train_cols]"]},{"cell_type":"code","execution_count":82,"metadata":{"execution":{"iopub.execute_input":"2022-04-09T17:54:34.517968Z","iopub.status.busy":"2022-04-09T17:54:34.517622Z","iopub.status.idle":"2022-04-09T17:54:34.529026Z","shell.execute_reply":"2022-04-09T17:54:34.528073Z","shell.execute_reply.started":"2022-04-09T17:54:34.517939Z"},"trusted":true},"outputs":[],"source":["train[[\"channel\", \"DOVE\", \"LUX\", \"VASELINE\"]] = train[[\"channel\", \"DOVE\", \"LUX\", \"VASELINE\"]].astype(int)\n","train.dtypes"]},{"cell_type":"code","execution_count":83,"metadata":{"execution":{"iopub.execute_input":"2022-04-09T17:54:35.577908Z","iopub.status.busy":"2022-04-09T17:54:35.577516Z","iopub.status.idle":"2022-04-09T17:54:35.586821Z","shell.execute_reply":"2022-04-09T17:54:35.585710Z","shell.execute_reply.started":"2022-04-09T17:54:35.577874Z"},"trusted":true},"outputs":[],"source":["samples = samples.astype(train.dtypes)"]},{"cell_type":"markdown","metadata":{},"source":["# modeling and evaluation"]},{"cell_type":"code","execution_count":119,"metadata":{"execution":{"iopub.execute_input":"2022-04-09T18:08:55.669642Z","iopub.status.busy":"2022-04-09T18:08:55.668964Z","iopub.status.idle":"2022-04-09T18:08:55.739480Z","shell.execute_reply":"2022-04-09T18:08:55.738497Z","shell.execute_reply.started":"2022-04-09T18:08:55.669587Z"},"trusted":true},"outputs":[],"source":["import lightgbm as lgb\n","from sklearn.metrics import mean_squared_error\n","\n","X_train, y_train = samples.drop(\"sales_value\", axis=1), samples.sales_value\n","X_test, y_test = train.drop(\"sales_value\", axis=1), train.sales_value\n","\n","train_data = lgb.Dataset(X_train, label=y_train)\n","validation_data = lgb.Dataset(X_test, label=y_test)\n","\n","params = {\n","    'boosting_type': \"goss\",\n","    'learning_rate': 0.05,\n","    'max_depth': 20,\n","    'num_leaves': 3,\n","#     'num_boost_round': 10,\n","    'force_col_wise': True,\n","    'objective': 'regression',  # 目标函数\n","    'verbose': 0\n","}\n","\n","gbm = lgb.train(params, train_data, valid_sets=[validation_data])\n","\n","# \n","y_pred = gbm.predict(X_test)\n","print(y_pred)\n","\n","# eval\n","print(mean_squared_error(y_test, y_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"}},"nbformat":4,"nbformat_minor":4}
